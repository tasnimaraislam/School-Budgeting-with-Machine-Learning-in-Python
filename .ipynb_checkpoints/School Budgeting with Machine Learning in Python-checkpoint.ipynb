{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description:\n",
    "\n",
    "Data science isn't just for predicting ad-clicks-it's also useful for social impact! This course is a case study from a machine learning competition on DrivenData. We'll explore a problem related to school district budgeting. By building a model to automatically classify items in a school's budget, it makes it easier and faster for schools to compare their spending with other schools. In this course, we'll begin by building a baseline model that is a simple, first-pass approach. In particular, we'll do some natural language processing to prepare the budgets for modeling. Next, we'll have the opportunity to try our own techniques and see how they compare to participants from the competition. Finally, we'll see how the winner was able to combine a number of expert techniques to build the most accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What type of machine learning problem is this?\n",
    "- Supervised Learning, because the model will be trained using labeled examples.\n",
    "\n",
    "### What is the goal of the algorithm? \n",
    "#### (The goal is to correctly label budget line items by training a supervised model to predict the probability of each possible label, taking most probable label as the correct label.)\n",
    "\n",
    "- Classification, because predicted probabilities will be used to select a label class. Specifically, we have ourselves a multi-class-multi-label classification problem, because there are 9 broad categories that each take on many possible sub-label instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "#### https://blog.csdn.net/u011292816/article/details/97391579\n",
    "#### https://blog.csdn.net/u011292816/article/details/97865973"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TrainingData.csv', <http.client.HTTPMessage at 0x210c64a5708>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_2533/datasets/TrainingSetSample.csv'\n",
    "from urllib.request import urlretrieve\n",
    "urlretrieve(fn, 'TrainingData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('TrainingData.csv', index_col=0)\n",
    "df.to_csv('TrainingData.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Function</th>\n",
       "      <th>Use</th>\n",
       "      <th>Sharing</th>\n",
       "      <th>Reporting</th>\n",
       "      <th>Student_Type</th>\n",
       "      <th>Position_Type</th>\n",
       "      <th>Object_Type</th>\n",
       "      <th>Pre_K</th>\n",
       "      <th>Operating_Status</th>\n",
       "      <th>Object_Description</th>\n",
       "      <th>...</th>\n",
       "      <th>Sub_Object_Description</th>\n",
       "      <th>Location_Description</th>\n",
       "      <th>FTE</th>\n",
       "      <th>Function_Description</th>\n",
       "      <th>Facility_or_Department</th>\n",
       "      <th>Position_Extra</th>\n",
       "      <th>Total</th>\n",
       "      <th>Program_Description</th>\n",
       "      <th>Fund_Description</th>\n",
       "      <th>Text_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Non-Operating</td>\n",
       "      <td>Supplemental *</td>\n",
       "      <td>...</td>\n",
       "      <td>Non-Certificated Salaries And Wages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Care and Upkeep of Building Services</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-8291.86000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Title I - Disadvantaged Children/Targeted Assi...</td>\n",
       "      <td>TITLE I CARRYOVER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Student Transportation</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Shared Services</td>\n",
       "      <td>Non-School</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Other Non-Compensation</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>REPAIR AND MAINTENANCE SERVICES</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ADMIN. SERVICES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STUDENT TRANSPORT SERVICE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>618.29000</td>\n",
       "      <td>PUPIL TRANSPORTATION</td>\n",
       "      <td>General Fund</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>Teacher Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>Base Salary/Compensation</td>\n",
       "      <td>Non PreK</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>Personal Services - Teachers</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TEACHER</td>\n",
       "      <td>49768.82000</td>\n",
       "      <td>Instruction - Regular</td>\n",
       "      <td>General Purpose School</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Non-Operating</td>\n",
       "      <td>General Supplies</td>\n",
       "      <td>...</td>\n",
       "      <td>General Supplies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>Instruction And Curriculum</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.02000</td>\n",
       "      <td>\"Title I, Part A Schoolwide Activities Related...</td>\n",
       "      <td>General Operating Fund</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Non-Operating</td>\n",
       "      <td>Supplies and Materials</td>\n",
       "      <td>...</td>\n",
       "      <td>Supplies And Materials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other Community Services *</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2304.43000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Title I - Disadvantaged Children/Targeted Assi...</td>\n",
       "      <td>TITLE I PI+HOMELESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344986</th>\n",
       "      <td>Substitute Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Substitute</td>\n",
       "      <td>Benefits</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>EMPLOYEE BENEFITS</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNALLOC BUDGETS/SCHOOLS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PROFESSIONAL-INSTRUCTIONAL</td>\n",
       "      <td>27.04000</td>\n",
       "      <td>GENERAL HIGH SCHOOL EDUCATION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REGULAR INSTRUCTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384803</th>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Non-Operating</td>\n",
       "      <td>EMPLOYEE BENEFITS</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PERSONNEL-PAID LEAVE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NON-PROJECT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PROFESSIONAL-INSTRUCTIONAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STAFF SERVICES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CENTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224382</th>\n",
       "      <td>Substitute Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>Special Education</td>\n",
       "      <td>Substitute</td>\n",
       "      <td>Substitute Compensation</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>OTHER PERSONAL SERVICES</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>School</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EXCEPTIONAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200.39000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GENERAL FUND</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305347</th>\n",
       "      <td>Facilities &amp; Maintenance</td>\n",
       "      <td>O&amp;M</td>\n",
       "      <td>Leadership &amp; Management</td>\n",
       "      <td>Non-School</td>\n",
       "      <td>Gifted</td>\n",
       "      <td>Custodian</td>\n",
       "      <td>Other Compensation/Stipend</td>\n",
       "      <td>Non PreK</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>Extra Duty Pay/Overtime For Support Personnel</td>\n",
       "      <td>...</td>\n",
       "      <td>Extra Duty Pay/Overtime For Support Personnel</td>\n",
       "      <td>Unallocated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Facilities Maintenance And Operations</td>\n",
       "      <td>Gifted And Talented</td>\n",
       "      <td>ANY CUS WHO IS NOT A SUPER</td>\n",
       "      <td>5.29000</td>\n",
       "      <td>Gifted And Talented</td>\n",
       "      <td>General Operating Fund</td>\n",
       "      <td>ADDL REGULAR PAY-NOT SMOOTHED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101861</th>\n",
       "      <td>Teacher Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>Poverty</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>Base Salary/Compensation</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>SALARIES OF REGULAR EMPLOYEES</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TITLE I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PROFESSIONAL-INSTRUCTIONAL</td>\n",
       "      <td>1575.03504</td>\n",
       "      <td>GENERAL ELEMENTARY EDUCATION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REGULAR INSTRUCTION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1560 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Function          Use                  Sharing  \\\n",
       "198                     NO_LABEL     NO_LABEL                 NO_LABEL   \n",
       "209       Student Transportation     NO_LABEL          Shared Services   \n",
       "750         Teacher Compensation  Instruction          School Reported   \n",
       "931                     NO_LABEL     NO_LABEL                 NO_LABEL   \n",
       "1524                    NO_LABEL     NO_LABEL                 NO_LABEL   \n",
       "...                          ...          ...                      ...   \n",
       "344986   Substitute Compensation  Instruction          School Reported   \n",
       "384803                  NO_LABEL     NO_LABEL                 NO_LABEL   \n",
       "224382   Substitute Compensation  Instruction          School Reported   \n",
       "305347  Facilities & Maintenance          O&M  Leadership & Management   \n",
       "101861      Teacher Compensation  Instruction          School Reported   \n",
       "\n",
       "         Reporting       Student_Type Position_Type  \\\n",
       "198       NO_LABEL           NO_LABEL      NO_LABEL   \n",
       "209     Non-School           NO_LABEL      NO_LABEL   \n",
       "750         School        Unspecified       Teacher   \n",
       "931       NO_LABEL           NO_LABEL      NO_LABEL   \n",
       "1524      NO_LABEL           NO_LABEL      NO_LABEL   \n",
       "...            ...                ...           ...   \n",
       "344986      School        Unspecified    Substitute   \n",
       "384803    NO_LABEL           NO_LABEL      NO_LABEL   \n",
       "224382      School  Special Education    Substitute   \n",
       "305347  Non-School             Gifted     Custodian   \n",
       "101861      School            Poverty       Teacher   \n",
       "\n",
       "                       Object_Type     Pre_K   Operating_Status  \\\n",
       "198                       NO_LABEL  NO_LABEL      Non-Operating   \n",
       "209         Other Non-Compensation  NO_LABEL  PreK-12 Operating   \n",
       "750       Base Salary/Compensation  Non PreK  PreK-12 Operating   \n",
       "931                       NO_LABEL  NO_LABEL      Non-Operating   \n",
       "1524                      NO_LABEL  NO_LABEL      Non-Operating   \n",
       "...                            ...       ...                ...   \n",
       "344986                    Benefits  NO_LABEL  PreK-12 Operating   \n",
       "384803                    NO_LABEL  NO_LABEL      Non-Operating   \n",
       "224382     Substitute Compensation  NO_LABEL  PreK-12 Operating   \n",
       "305347  Other Compensation/Stipend  Non PreK  PreK-12 Operating   \n",
       "101861    Base Salary/Compensation  NO_LABEL  PreK-12 Operating   \n",
       "\n",
       "                                   Object_Description  ...  \\\n",
       "198                                    Supplemental *  ...   \n",
       "209                   REPAIR AND MAINTENANCE SERVICES  ...   \n",
       "750                      Personal Services - Teachers  ...   \n",
       "931                                  General Supplies  ...   \n",
       "1524                           Supplies and Materials  ...   \n",
       "...                                               ...  ...   \n",
       "344986                              EMPLOYEE BENEFITS  ...   \n",
       "384803                              EMPLOYEE BENEFITS  ...   \n",
       "224382                 OTHER PERSONAL SERVICES         ...   \n",
       "305347  Extra Duty Pay/Overtime For Support Personnel  ...   \n",
       "101861                  SALARIES OF REGULAR EMPLOYEES  ...   \n",
       "\n",
       "                               Sub_Object_Description  Location_Description  \\\n",
       "198               Non-Certificated Salaries And Wages                   NaN   \n",
       "209                                               NaN       ADMIN. SERVICES   \n",
       "750                                               NaN                   NaN   \n",
       "931                                  General Supplies                   NaN   \n",
       "1524                           Supplies And Materials                   NaN   \n",
       "...                                               ...                   ...   \n",
       "344986                                            NaN                   NaN   \n",
       "384803                                            NaN  PERSONNEL-PAID LEAVE   \n",
       "224382                                            NaN               School    \n",
       "305347  Extra Duty Pay/Overtime For Support Personnel           Unallocated   \n",
       "101861                                            NaN                   NaN   \n",
       "\n",
       "        FTE                   Function_Description  \\\n",
       "198     NaN   Care and Upkeep of Building Services   \n",
       "209     NaN              STUDENT TRANSPORT SERVICE   \n",
       "750     1.0                                    NaN   \n",
       "931     NaN                            Instruction   \n",
       "1524    NaN             Other Community Services *   \n",
       "...     ...                                    ...   \n",
       "344986  NaN                UNALLOC BUDGETS/SCHOOLS   \n",
       "384803  NaN                            NON-PROJECT   \n",
       "224382  0.0         EXCEPTIONAL                      \n",
       "305347  NaN  Facilities Maintenance And Operations   \n",
       "101861  NaN                                TITLE I   \n",
       "\n",
       "            Facility_or_Department               Position_Extra        Total  \\\n",
       "198                            NaN                          NaN  -8291.86000   \n",
       "209                            NaN                          NaN    618.29000   \n",
       "750                            NaN                      TEACHER  49768.82000   \n",
       "931     Instruction And Curriculum                          NaN     -1.02000   \n",
       "1524                           NaN                          NaN   2304.43000   \n",
       "...                            ...                          ...          ...   \n",
       "344986                         NaN   PROFESSIONAL-INSTRUCTIONAL     27.04000   \n",
       "384803                         NaN   PROFESSIONAL-INSTRUCTIONAL          NaN   \n",
       "224382                         NaN                          NaN    200.39000   \n",
       "305347         Gifted And Talented  ANY CUS WHO IS NOT A SUPER       5.29000   \n",
       "101861                         NaN   PROFESSIONAL-INSTRUCTIONAL   1575.03504   \n",
       "\n",
       "                                      Program_Description  \\\n",
       "198                                                   NaN   \n",
       "209                                  PUPIL TRANSPORTATION   \n",
       "750                                 Instruction - Regular   \n",
       "931     \"Title I, Part A Schoolwide Activities Related...   \n",
       "1524                                                  NaN   \n",
       "...                                                   ...   \n",
       "344986                      GENERAL HIGH SCHOOL EDUCATION   \n",
       "384803                                     STAFF SERVICES   \n",
       "224382                                                NaN   \n",
       "305347                                Gifted And Talented   \n",
       "101861                       GENERAL ELEMENTARY EDUCATION   \n",
       "\n",
       "                                         Fund_Description  \\\n",
       "198     Title I - Disadvantaged Children/Targeted Assi...   \n",
       "209                                          General Fund   \n",
       "750                                General Purpose School   \n",
       "931                                General Operating Fund   \n",
       "1524    Title I - Disadvantaged Children/Targeted Assi...   \n",
       "...                                                   ...   \n",
       "344986                                                NaN   \n",
       "384803                                                NaN   \n",
       "224382                     GENERAL FUND                     \n",
       "305347                             General Operating Fund   \n",
       "101861                                                NaN   \n",
       "\n",
       "                               Text_1  \n",
       "198                TITLE I CARRYOVER   \n",
       "209                               NaN  \n",
       "750                               NaN  \n",
       "931                               NaN  \n",
       "1524              TITLE I PI+HOMELESS  \n",
       "...                               ...  \n",
       "344986            REGULAR INSTRUCTION  \n",
       "384803                        CENTRAL  \n",
       "224382                            NaN  \n",
       "305347  ADDL REGULAR PAY-NOT SMOOTHED  \n",
       "101861            REGULAR INSTRUCTION  \n",
       "\n",
       "[1560 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1560 entries, 198 to 101861\n",
      "Data columns (total 25 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Function                1560 non-null   object \n",
      " 1   Use                     1560 non-null   object \n",
      " 2   Sharing                 1560 non-null   object \n",
      " 3   Reporting               1560 non-null   object \n",
      " 4   Student_Type            1560 non-null   object \n",
      " 5   Position_Type           1560 non-null   object \n",
      " 6   Object_Type             1560 non-null   object \n",
      " 7   Pre_K                   1560 non-null   object \n",
      " 8   Operating_Status        1560 non-null   object \n",
      " 9   Object_Description      1461 non-null   object \n",
      " 10  Text_2                  382 non-null    object \n",
      " 11  SubFund_Description     1183 non-null   object \n",
      " 12  Job_Title_Description   1131 non-null   object \n",
      " 13  Text_3                  296 non-null    object \n",
      " 14  Text_4                  193 non-null    object \n",
      " 15  Sub_Object_Description  364 non-null    object \n",
      " 16  Location_Description    874 non-null    object \n",
      " 17  FTE                     449 non-null    float64\n",
      " 18  Function_Description    1340 non-null   object \n",
      " 19  Facility_or_Department  252 non-null    object \n",
      " 20  Position_Extra          1026 non-null   object \n",
      " 21  Total                   1542 non-null   float64\n",
      " 22  Program_Description     1192 non-null   object \n",
      " 23  Fund_Description        819 non-null    object \n",
      " 24  Text_1                  1132 non-null   object \n",
      "dtypes: float64(2), object(23)\n",
      "memory usage: 316.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing the data\n",
    "There are two numeric columns, called FTE and Total.\n",
    "\n",
    "- FTE: Stands for \"full-time equivalent\". If the budget item is associated to an employee, this number tells us the percentage of full-time that the employee works. A value of 1 means the associated employee works for the school full-time. A value close to 0 means the item is associated to a part-time or contracted employee.\n",
    "\n",
    "\n",
    "- Total: Stands for the total cost of the expenditure. This number tells us how much the budget item cost.\n",
    "\n",
    "Now job is to plot a histogram of the non-null FTE column to see the distribution of part-time and full-time employees in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              FTE         Total\n",
      "count  449.000000  1.542000e+03\n",
      "mean     0.493532  1.446867e+04\n",
      "std      0.452844  7.916752e+04\n",
      "min     -0.002369 -1.044084e+06\n",
      "25%      0.004310  1.108111e+02\n",
      "50%      0.440000  7.060299e+02\n",
      "75%      1.000000  5.347760e+03\n",
      "max      1.047222  1.367500e+06\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the summary statistics\n",
    "print(df.describe())\n",
    "\n",
    "# Import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the histogram\n",
    "plt.hist(df['FTE'].dropna())\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Distribution of %full-time \\n employee works')\n",
    "plt.xlabel('% of full-time')\n",
    "plt.ylabel('num employees')\n",
    "\n",
    "# Display the histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The high variance in expenditures makes sense (some purchases are cheap some are expensive). Also, it looks like the FTE column is bimodal. That is, there are some part-time and some full-time employees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring datatypes in pandas\n",
    "\n",
    "It's always good to know what datatypes we're working with, especially when the inefficient pandas type object may be involved. Towards that end, let's explore what we have.\n",
    "- look at the DataFrame attribute \n",
    "- call df.dtypes.value_counts(), and not df.value_counts()! Check out the difference in the Shell. df.value_counts() will return an error, because it is a Series method, not a DataFrame method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "object     23\n",
       "float64     2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the labels as categorical variables\n",
    "The ultimate goal is to predict the probability that a certain label is attached to a budget line item. we just saw that many columns in the data are the inefficient object type. Does this include the labels we're trying to predict? Let's find out!\n",
    "\n",
    "There are 9 columns of labels in the dataset. Each of these columns is a category that has many possible values it can take. The 9 labels have been loaded into a list called LABELS. \n",
    "\n",
    "Every label is encoded as an object datatype. Because category datatypes are much more efficient our task is to convert the labels to category types using the .astype() method.\n",
    "\n",
    "Note: .astype() only works on a pandas Series. Since we are working with a pandas DataFrame, we'll need to use the .apply() method and provide a lambda function called categorize_label that applies .astype() to each column, x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Function</th>\n",
       "      <th>Use</th>\n",
       "      <th>Sharing</th>\n",
       "      <th>Reporting</th>\n",
       "      <th>Student_Type</th>\n",
       "      <th>Position_Type</th>\n",
       "      <th>Object_Type</th>\n",
       "      <th>Pre_K</th>\n",
       "      <th>Operating_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Non-Operating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Student Transportation</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Shared Services</td>\n",
       "      <td>Non-School</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Other Non-Compensation</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>Teacher Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>Base Salary/Compensation</td>\n",
       "      <td>Non PreK</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Non-Operating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Non-Operating</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Function          Use          Sharing   Reporting  \\\n",
       "198                 NO_LABEL     NO_LABEL         NO_LABEL    NO_LABEL   \n",
       "209   Student Transportation     NO_LABEL  Shared Services  Non-School   \n",
       "750     Teacher Compensation  Instruction  School Reported      School   \n",
       "931                 NO_LABEL     NO_LABEL         NO_LABEL    NO_LABEL   \n",
       "1524                NO_LABEL     NO_LABEL         NO_LABEL    NO_LABEL   \n",
       "\n",
       "     Student_Type Position_Type               Object_Type     Pre_K  \\\n",
       "198      NO_LABEL      NO_LABEL                  NO_LABEL  NO_LABEL   \n",
       "209      NO_LABEL      NO_LABEL    Other Non-Compensation  NO_LABEL   \n",
       "750   Unspecified       Teacher  Base Salary/Compensation  Non PreK   \n",
       "931      NO_LABEL      NO_LABEL                  NO_LABEL  NO_LABEL   \n",
       "1524     NO_LABEL      NO_LABEL                  NO_LABEL  NO_LABEL   \n",
       "\n",
       "       Operating_Status  \n",
       "198       Non-Operating  \n",
       "209   PreK-12 Operating  \n",
       "750   PreK-12 Operating  \n",
       "931       Non-Operating  \n",
       "1524      Non-Operating  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS = [\"Function\",\"Use\", \"Sharing\", \"Reporting\", \"Student_Type\", \"Position_Type\", \"Object_Type\", \"Pre_K\", \"Operating_Status\" ]\n",
    "df[LABELS].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function            object\n",
       "Use                 object\n",
       "Sharing             object\n",
       "Reporting           object\n",
       "Student_Type        object\n",
       "Position_Type       object\n",
       "Object_Type         object\n",
       "Pre_K               object\n",
       "Operating_Status    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[LABELS].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function            category\n",
      "Use                 category\n",
      "Sharing             category\n",
      "Reporting           category\n",
      "Student_Type        category\n",
      "Position_Type       category\n",
      "Object_Type         category\n",
      "Pre_K               category\n",
      "Operating_Status    category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Define the lambda function: categorize_label\n",
    "categorize_label = lambda x: x.astype('category')\n",
    "\n",
    "# Convert df[LABELS] to a categorical type\n",
    "df[LABELS] = df[LABELS].apply(categorize_label)\n",
    "\n",
    "# Print the converted dtypes\n",
    "print(df[LABELS].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting unique labels\n",
    "There are over 100 unique labels. Now, we will explore this fact by counting and plotting the number of unique values for each category of label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate number of unique values for each label: num_unique_labels\n",
    "num_unique_labels = df[LABELS].apply(pd.Series.nunique)\n",
    "\n",
    "# Plot number of unique values for each label\n",
    "num_unique_labels.plot(kind='bar')\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Number of unique values')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing log loss with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define LogLoss function, account for clipping\n",
    "\n",
    "def compute_log_loss(predicted, actual, eps=1e-14):    \n",
    "    predicted = np.clip(predicted, eps, 1 - eps)\n",
    "    return -1 * np.mean(actual * np.log(predicted) + (1 - actual) * np.log(1 - predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataDriven provided function\n",
    "\n",
    "from warnings import warn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def multilabel_sample(y, size=1000, min_count=5, seed=None):\n",
    "    \"\"\" Takes a matrix of binary labels `y` and returns\n",
    "        the indices for a sample of size `size` if\n",
    "        `size` > 1 or `size` * len(y) if size =< 1.\n",
    "        The sample is guaranteed to have > `min_count` of\n",
    "        each label.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if (np.unique(y).astype(int) != np.array([0, 1])).any():\n",
    "            raise ValueError()\n",
    "    except (TypeError, ValueError):\n",
    "        raise ValueError('multilabel_sample only works with binary indicator matrices')\n",
    "\n",
    "    if (y.sum(axis=0) < min_count).any():\n",
    "        raise ValueError('Some classes do not have enough examples. Change min_count if necessary.')\n",
    "\n",
    "    if size <= 1:\n",
    "        size = np.floor(y.shape[0] * size)\n",
    "\n",
    "    if y.shape[1] * min_count > size:\n",
    "        msg = \"Size less than number of columns * min_count, returning {} items instead of {}.\"\n",
    "        warn(msg.format(y.shape[1] * min_count, size))\n",
    "        size = y.shape[1] * min_count\n",
    "\n",
    "    rng = np.random.RandomState(seed if seed is not None else np.random.randint(1))\n",
    "\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        choices = y.index\n",
    "        y = y.values\n",
    "    else:\n",
    "        choices = np.arange(y.shape[0])\n",
    "\n",
    "    sample_idxs = np.array([], dtype=choices.dtype)\n",
    "\n",
    "    # first, guarantee > min_count of each label\n",
    "    for j in range(y.shape[1]):\n",
    "        label_choices = choices[y[:, j] == 1]\n",
    "        label_idxs_sampled = rng.choice(label_choices, size=min_count, replace=False)\n",
    "        sample_idxs = np.concatenate([label_idxs_sampled, sample_idxs])\n",
    "\n",
    "    sample_idxs = np.unique(sample_idxs)\n",
    "\n",
    "    # now that we have at least min_count of each, we can just random sample\n",
    "    sample_count = int(size - sample_idxs.shape[0])\n",
    "\n",
    "    # get sample_count indices from remaining choices\n",
    "    remaining_choices = np.setdiff1d(choices, sample_idxs)\n",
    "    remaining_sampled = rng.choice(remaining_choices,\n",
    "                                   size=sample_count,\n",
    "                                   replace=False)\n",
    "\n",
    "    return np.concatenate([sample_idxs, remaining_sampled])\n",
    "\n",
    "\n",
    "def multilabel_sample_dataframe(df, labels, size, min_count=5, seed=None):\n",
    "    \"\"\" Takes a dataframe `df` and returns a sample of size `size` where all\n",
    "        classes in the binary matrix `labels` are represented at\n",
    "        least `min_count` times.\n",
    "    \"\"\"\n",
    "    idxs = multilabel_sample(labels, size=size, min_count=min_count, seed=seed)\n",
    "    return df.loc[idxs]\n",
    "\n",
    "\n",
    "def multilabel_train_test_split(X, Y, size, min_count=5, seed=None):\n",
    "    \"\"\" Takes a features matrix `X` and a label matrix `Y` and\n",
    "        returns (X_train, X_test, Y_train, Y_test) where all\n",
    "        classes in Y are represented at least `min_count` times.\n",
    "    \"\"\"\n",
    "    index = Y.index if isinstance(Y, pd.DataFrame) else np.arange(Y.shape[0])\n",
    "\n",
    "    test_set_idxs = multilabel_sample(Y, size=size, min_count=min_count, seed=seed)\n",
    "    train_set_idxs = np.setdiff1d(index, test_set_idxs)\n",
    "\n",
    "    test_set_mask = index.isin(test_set_idxs)\n",
    "    train_set_mask = ~test_set_mask\n",
    "\n",
    "    return (X[train_set_mask], X[test_set_mask], Y[train_set_mask], Y[test_set_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1040 entries, 198 to 101861\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   FTE     1040 non-null   float64\n",
      " 1   Total   1040 non-null   float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 24.4 KB\n",
      "None\n",
      "\n",
      "X_test info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 520 entries, 209 to 448628\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   FTE     520 non-null    float64\n",
      " 1   Total   520 non-null    float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 12.2 KB\n",
      "None\n",
      "\n",
      "y_train info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1040 entries, 198 to 101861\n",
      "Columns: 104 entries, Function_Aides Compensation to Operating_Status_PreK-12 Operating\n",
      "dtypes: uint8(104)\n",
      "memory usage: 113.8 KB\n",
      "None\n",
      "\n",
      "y_test info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 520 entries, 209 to 448628\n",
      "Columns: 104 entries, Function_Aides Compensation to Operating_Status_PreK-12 Operating\n",
      "dtypes: uint8(104)\n",
      "memory usage: 56.9 KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tasni\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Size less than number of columns * min_count, returning 520 items instead of 312.0.\n"
     ]
    }
   ],
   "source": [
    "NUMERIC_COLUMNS = ['FTE', 'Total']\n",
    "# Create the new DataFrame: numeric_data_only\n",
    "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
    "\n",
    "# Get labels and convert to dummy variables: label_dummies\n",
    "label_dummies = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,label_dummies,size=0.2,seed=123)\n",
    "\n",
    "# Print the info\n",
    "print(\"X_train info:\")\n",
    "print(X_train.info())\n",
    "print(\"\\nX_test info:\")  \n",
    "print(X_test.info())\n",
    "print(\"\\ny_train info:\")  \n",
    "print(y_train.info())\n",
    "print(\"\\ny_test info:\")  \n",
    "print(y_test.info()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model\n",
    "We will import the logistic regression and one versus rest classifiers in order to fit a multi-class logistic regression model to the NUMERIC_COLUMNS of our feature data.\n",
    "\n",
    "Then we'll test and print the accuracy with the .score() method to see the results of training.\n",
    "\n",
    "We're ultimately going to be using logloss to score our model, so don't worry too much about the accuracy here. Keep in mind that we're throwing away all of the text data in the dataset - that's by far most of the data! So don't get hopes up for a killer performance just yet. We're just interested in getting things up and running at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Import classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Instantiate the classifier: clf\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The good news is that our workflow didn't cause any errors. The bad news is that our model scored the lowest possible accuracy: 0.0! But hey, we just threw away ALL of the text data in the budget. Later, we won't. Before we add the text data, let's see how the model does when scored by log loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model to predict values on holdout data\n",
    "We're ready to make some predictions! Remember, the train-test-split we've carried out so far is for model development. The original competition provides an additional test set, for which we'll never actually see the correct labels. This is called the \"holdout data.\"\n",
    "\n",
    "The point of the holdout data is to provide a fair test for machine learning competitions. If the labels aren't known by anyone but DataCamp, DrivenData, or whoever is hosting the competition, we can be sure that no one submits a mere copy of labels to artificially pump up the performance on their model.\n",
    "\n",
    "Remember that the original goal is to predict the probability of each label. Now, we'll do just that by using the .predict_proba() method on our trained model.\n",
    "\n",
    "First, however, we'll need to load the holdout data, which is available in the workspace as the file HoldoutData.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('HoldoutData.csv', <http.client.HTTPMessage at 0x210c8bc5708>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://blog.csdn.net/u011292816/article/details/97444251\n",
    "\n",
    "# Load path to holdout\n",
    "\n",
    "PATH_TO_HOLDOUT_DATA = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_2826/datasets/TestSetSample.csv'\n",
    "PATH_TO_HOLDOUT_LABELS = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_2826/datasets/TestSetLabelsSample.csv'\n",
    "fn = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_2826/datasets/TestSetSample.csv'\n",
    "from urllib.request import urlretrieve\n",
    "urlretrieve(fn, 'HoldoutData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2000 entries, 237 to 142060\n",
      "Data columns (total 16 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Object_Description      1918 non-null   object \n",
      " 1   Program_Description     1798 non-null   object \n",
      " 2   SubFund_Description     664 non-null    object \n",
      " 3   Job_Title_Description   1303 non-null   object \n",
      " 4   Facility_or_Department  133 non-null    object \n",
      " 5   Sub_Object_Description  1317 non-null   object \n",
      " 6   Location_Description    1515 non-null   object \n",
      " 7   FTE                     782 non-null    float64\n",
      " 8   Function_Description    1851 non-null   object \n",
      " 9   Position_Extra          594 non-null    object \n",
      " 10  Text_4                  137 non-null    object \n",
      " 11  Total                   1974 non-null   float64\n",
      " 12  Text_2                  198 non-null    object \n",
      " 13  Text_3                  216 non-null    object \n",
      " 14  Fund_Description        1559 non-null   object \n",
      " 15  Text_1                  627 non-null    object \n",
      "dtypes: float64(2), object(14)\n",
      "memory usage: 265.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the classifier: clf\n",
    "clf = OneVsRestClassifier(LogisticRegression(solver='liblinear'))\n",
    "\n",
    "# Fit it to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Load the holdout data: holdout\n",
    "holdout = pd.read_csv('HoldoutData.csv',index_col=0)\n",
    "holdout.to_csv(\"HoldoutData.csv\")\n",
    "print(holdout.info())\n",
    "\n",
    "# Generate predictions: predictions\n",
    "predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing out our results to a csv for submission\n",
    "At last, we're ready to submit some predictions for scoring. Now, we'll write our predictions to a .csv using the .to_csv() method on a pandas DataFrame. Then we'll evaluate our performance according to the LogLoss metric!\n",
    "\n",
    "We'll need to make sure our submission obeys the correct format (https://www.drivendata.org/competitions/4/page/15/#sub_values).\n",
    "\n",
    "To do this, we'll use our predictions values to create a new DataFrame, prediction_df.\n",
    "\n",
    "Interpreting LogLoss & Beating the Benchmark:\n",
    "\n",
    "When interpreting log loss score, keep in mind that the score will change based on the number of samples tested. To get a sense of how this very basic model performs, compare our score to the DrivenData benchmark model performance: 2.0455, which merely submitted uniform probabilities for each class.\n",
    "\n",
    "Remember, the lower the log loss the better. \n",
    "\n",
    "Is our model's log loss lower than 2.0455?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataDriven provided function\n",
    "\n",
    "# Load path to pred\n",
    "PATH_TO_PREDICTIONS = \"predictions.csv\"\n",
    "# Load path to holdout\n",
    "PATH_TO_HOLDOUT_DATA = \"https://s3.amazonaws.com/assets.datacamp.com/production/course_2826/datasets/TestSetSample.csv\"\n",
    "PATH_TO_HOLDOUT_LABELS = \"https://s3.amazonaws.com/assets.datacamp.com/production/course_2826/datasets/TestSetLabelsSample.csv\"\n",
    "\n",
    "# SCORING UTILITIES\n",
    "\n",
    "BOX_PLOTS_COLUMN_INDICES = [range(37),\n",
    " range(37,48),\n",
    " range(48,51),\n",
    " range(51,76),\n",
    " range(76,79),\n",
    " range(79,82),\n",
    " range(82,87),\n",
    " range(87,96),\n",
    " range(96,104)]\n",
    "\n",
    "def _multi_multi_log_loss(predicted,\n",
    "    actual,\n",
    "    class_column_indices=BOX_PLOTS_COLUMN_INDICES,\n",
    "    eps=1e-15):\n",
    "\n",
    "    class_scores = np.ones(len(class_column_indices), dtype=np.float64)\n",
    "\n",
    "    # calculate log loss for each set of columns that belong to a class:\n",
    "    for k, this_class_indices in enumerate(class_column_indices):\n",
    "        # get just the columns for this class\n",
    "        preds_k = predicted[:, this_class_indices].astype(np.float64)\n",
    "\n",
    "        # normalize so probabilities sum to one (unless sum is zero, then we clip)\n",
    "        preds_k /= np.clip(preds_k.sum(axis=1).reshape(-1, 1), eps, np.inf)\n",
    "\n",
    "        actual_k = actual[:, this_class_indices]\n",
    "\n",
    "        # shrink predictions so\n",
    "        y_hats = np.clip(preds_k, eps, 1 - eps)\n",
    "        sum_logs = np.sum(actual_k * np.log(y_hats))\n",
    "        class_scores[k] = (-1.0 / actual.shape[0]) * sum_logs\n",
    "\n",
    "        return np.average(class_scores)\n",
    "\n",
    "\n",
    "def score_submission(pred_path=PATH_TO_PREDICTIONS, holdout_path=PATH_TO_HOLDOUT_LABELS):\n",
    "    # this happens on the backend to get the score\n",
    "    holdout_labels = pd.get_dummies(\n",
    "    pd.read_csv(holdout_path, index_col=0)\n",
    "    .apply(lambda x: x.astype('category'), axis=0)\n",
    "    )\n",
    "\n",
    "    preds = pd.read_csv(pred_path, index_col=0)\n",
    "\n",
    "    # make sure that format is correct\n",
    "    assert (preds.columns == holdout_labels.columns).all()\n",
    "    assert (preds.index == holdout_labels.index).all()\n",
    "\n",
    "    return _multi_multi_log_loss(preds.values, holdout_labels.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model, trained with numeric data only, yields logloss score: 1.2355989666684484\n"
     ]
    }
   ],
   "source": [
    "# Format predictions in DataFrame: prediction_df\n",
    "prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS]).columns,\n",
    "                             index=holdout.index,\n",
    "                             data=predictions)\n",
    "\n",
    "# Save prediction_df to csv\n",
    "prediction_df.to_csv(\"predictions.csv\")\n",
    "\n",
    "# Submit the predictions for scoring: score\n",
    "score = score_submission(pred_path=\"predictions.csv\")\n",
    "\n",
    "# Print score\n",
    "print('Our model, trained with numeric data only, yields logloss score: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Even though our basic model scored 0.0 accuracy, it nevertheless performs better than the benchmark score of 2.0455. We've now got the basics down and have made a first pass at this complicated supervised learning problem. It's time to step up our game and incorporate the text data.\n",
    "\n",
    "From Datacamp, we get \n",
    " \n",
    "\"Our model, trained with numeric data only, yields logloss score: 1.9067227623381413\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a bag-of-words in scikit-learn\n",
    "Now, we'll study the effects of tokenizing in different ways by comparing the bag-of-words representations resulting from different token patterns.\n",
    "\n",
    "We will focus on one feature only, the Position_Extra column, which describes any additional information not captured by the Position_Type label.\n",
    "\n",
    "For example, we can check out the budget item in row 8960 of the data using df.loc[8960]. Looking at the output reveals that this Object_Description is overtime pay. For who? The Position Type is merely \"other\", but the Position Extra elaborates: \"BUS DRIVER\". Explore the column further to see more instances. It has a lot of NaN values.\n",
    "\n",
    "The task is to turn the raw text in this column into a bag-of-words representation by creating tokens that contain only alphanumeric characters.\n",
    "\n",
    "For comparison purposes, the first 15 tokens of vec_basic, which splits df.Position_Extra into tokens when it encounters only whitespace characters, have been printed along with the length of the representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 123 tokens in Position_Extra if we split on non-alpha numeric\n",
      "['1st', '2nd', '3rd', 'a', 'ab', 'additional', 'adm', 'administrative', 'and', 'any', 'art', 'assessment', 'assistant', 'asst', 'athletic']\n"
     ]
    }
   ],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Fill missing values in df.Position_Extra\n",
    "df.Position_Extra.fillna('', inplace = True) #argument inplace=True so that you don't have to assign the result back to df.\n",
    "\n",
    "# Instantiate the CountVectorizer: vec_alphanumeric\n",
    "vec_alphanumeric = CountVectorizer(token_pattern = TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Fit to the data\n",
    "vec_alphanumeric.fit(df.Position_Extra)\n",
    "\n",
    "# Print the number of tokens and first 15 tokens\n",
    "msg = \"There are {} tokens in Position_Extra if we split on non-alpha numeric\"\n",
    "print(msg.format(len(vec_alphanumeric.get_feature_names())))\n",
    "print(vec_alphanumeric.get_feature_names()[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Treating only alpha-numeric characters as tokens gives us a smaller number of more meaningful tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining text columns for tokenization\n",
    "In order to get a bag-of-words representation for all of the text data in our DataFrame, we must first convert the text data in each row of the DataFrame into a single string.\n",
    "\n",
    "In the previous exercise, this wasn't necessary because we only looked at one column of data, so each row was already just a single string. CountVectorizer expects each row to just be a single string, so in order to use all of the text columns, we'll need a method to turn a list of strings into a single string.\n",
    "\n",
    "In this exercise, we'll complete the function definition combine_text_columns(). When completed, this function will convert all training text data in our DataFrame to a single string per row that can be passed to the vectorizer object and made into a bag-of-words using the .fit_transform() method.\n",
    "\n",
    "Note that the function uses NUMERIC_COLUMNS and LABELS to determine which columns to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define combine_text_columns()\n",
    "def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):\n",
    "    \"\"\" converts all text in each row of data_frame to single vector \"\"\"\n",
    "    \n",
    "    # Drop non-text columns that are in the df\n",
    "    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
    "    text_data = data_frame.drop(to_drop, axis = 1)\n",
    "    \n",
    "    # Replace nans with blanks\n",
    "    text_data.fillna(\"\", inplace=True)\n",
    "    \n",
    "    # Join all text items in a row that have a space in between\n",
    "    return text_data.apply(lambda x: \" \".join(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's in a token?\n",
    "Now we will use combine_text_columns to convert all training text data in our DataFrame to a single vector that can be passed to the vectorizer object and made into a bag-of-words using the .fit_transform() method.\n",
    "\n",
    "We'll compare the effect of tokenizing using any non-whitespace characters as a token and using only alphanumeric characters as a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1404 tokens in the dataset\n",
      "There are 1117 alpha-numeric tokens in the dataset\n"
     ]
    }
   ],
   "source": [
    "# Import the CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the basic token pattern\n",
    "TOKENS_BASIC = '\\\\S+(?=\\\\s+)'\n",
    "\n",
    "# Create the alphanumeric token pattern\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate basic CountVectorizer: vec_basic\n",
    "vec_basic = CountVectorizer(token_pattern = TOKENS_BASIC)\n",
    "\n",
    "# Instantiate alphanumeric CountVectorizer: vec_alphanumeric\n",
    "vec_alphanumeric = CountVectorizer(token_pattern = TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Create the text vector\n",
    "text_vector = combine_text_columns(df)\n",
    "\n",
    "# Fit and transform vec_basic\n",
    "vec_basic.fit_transform(text_vector)\n",
    "\n",
    "# Print number of tokens of vec_basic\n",
    "print(\"There are {} tokens in the dataset\".format(len(vec_basic.get_feature_names())))\n",
    "\n",
    "# Fit and transform vec_alphanumeric\n",
    "vec_alphanumeric.fit_transform(text_vector)\n",
    "\n",
    "# Print number of tokens of vec_alphanumeric\n",
    "print(\"There are {} alpha-numeric tokens in the dataset\".format(len(vec_alphanumeric.get_feature_names())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice that tokenizing on alpha-numeric tokens reduced the number of tokens, just as in the last exercise. We'll keep this in mind when building a better model with the Pipeline object next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate pipeline\n",
    "In order to make our life easier as we start to work with all of the data in our original DataFrame, df, it's time to turn to one of scikit-learn's most useful objects: the Pipeline.\n",
    "### Preprocessing numeric features\n",
    "Without imputing missing values, the pipeline would not be happy. So, we'll improve our pipeline a bit by using the Imputer() imputation transformer from scikit-learn to fill in missing values in our data.\n",
    "\n",
    "By default, the imputer transformer replaces NaNs with the mean value of the column. That's a good enough imputation strategy for the data, so we won't need to pass anything extra to the imputer.\n",
    "\n",
    "After importing the transformer, we will edit the steps list used in the previous exercise by inserting a (name, transform) tuple. Recall that steps are processed sequentially, so make sure the new tuple encoding your preprocessing step is put in the right place.\n",
    "### Preprocessing text features\n",
    "To preprocess the text, we'll turn to CountVectorizer() to generate a bag-of-words representation of the data. Using the default arguments, add a (step, transform) tuple to the steps list in our pipeline.\n",
    "\n",
    "Make sure to select only the text column for splitting training and test sets.\n",
    "### Multiple types of processing: FunctionTransformer\n",
    "\n",
    "Any step in the pipeline must be an object that implements the fit and transform methods. The FunctionTransformer creates an object with these methods out of any Python function that you pass to it. We'll use it to help select subsets of data in a way that plays nicely with pipelines.\n",
    "\n",
    "We are working with numeric data that needs imputation, and text data that needs to be converted into a bag-of-words. We'll create functions that separate the text from the numeric variables and see how the .fit() and .transform() methods work.\n",
    "### Multiple types of processing: FeatureUnion\n",
    "Now that you can separate text and numeric data in your pipeline, you're ready to perform separate steps on each by nesting pipelines and using FeatureUnion().\n",
    "\n",
    "These tools will allow you to streamline all preprocessing steps for your model, even when multiple datatypes are involved. Here, for example, you don't want to impute our text data, and you don't want to create a bag-of-words with our numeric data. Instead, you want to deal with these separately and then join the results together using FeatureUnion().\n",
    "\n",
    "In the end, you'll still have only two high-level steps in your pipeline: preprocessing and model instantiation. The difference is that the first preprocessing step actually consists of a pipeline for numeric data and a pipeline for text data. The results of those pipelines are joined using FeatureUnion()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using FunctionTransformer on the main dataset\n",
    "Now we're going to use FunctionTransformer on the primary budget data, before instantiating a multiple-datatype pipeline.\n",
    "\n",
    "We need:\n",
    "- custom function combine_text_columns which was created to select and properly format text data for tokenization; \n",
    "- NUMERIC_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tasni\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Size less than number of columns * min_count, returning 520 items instead of 312.0.\n"
     ]
    }
   ],
   "source": [
    "# Import FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Get the dummy encoding of the labels\n",
    "dummy_labels = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Get the columns that are features in the original df\n",
    "NON_LABELS = [c for c in df.columns if c not in LABELS]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split((df[NON_LABELS]),dummy_labels,0.2,seed=123)\n",
    "\n",
    "# Preprocess the text data: get_text_data\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate = False)\n",
    "\n",
    "# Preprocess the numeric data: get_numeric_data\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a model to the pipeline\n",
    "We're about to implement a Pipeline that works with the real, DrivenData budget line item data we've been exploring.\n",
    "\n",
    "- the preprocessing step uses FeatureUnion to join the results of nested pipelines that each rely on FunctionTransformer to select multiple datatypes\n",
    "- the model step stores the model object\n",
    "\n",
    "We can then call familiar methods like .fit() and .score() on the Pipeline object pl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tasni\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\tasni\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Import the Imputer object\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Complete the pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', SimpleImputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At Datacamp we get, \n",
    "\n",
    "\"Accuracy on budget dataset:  0.20384615384615384\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that you've built the entire pipeline, you can easily start trying out different models by just modifying the 'clf' step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a different class of model\n",
    "One of the great strengths of pipelines is how easy they make the process of testing different models.\n",
    "\n",
    "Until now, we've been using the model step ('clf', OneVsRestClassifier(LogisticRegression())) in our pipeline.\n",
    "\n",
    "But what if we want to try a different model? Do we need to build an entirely new pipeline? New nests? New FeatureUnions? Nope! We just have a simple one-line change, as we'll see in this exercise.\n",
    "\n",
    "In particular, we'll swap out the logistic-regression model and replace it with a random forest classifier, which uses the statistics of an ensemble of decision trees to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.3211538461538462\n"
     ]
    }
   ],
   "source": [
    "# Import random forest classifer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Edit model step in pipeline\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', SimpleImputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At Datacamp we get, \n",
    "\n",
    "\"Accuracy on budget dataset:  0.2826923076923077\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you adjust the model or parameters to improve accuracy?\n",
    "We just saw a substantial improvement in accuracy by swapping out the model. Pipelines are amazing!\n",
    "\n",
    "Can we make it better? Try changing the parameter n_estimators of RandomForestClassifier(), whose default value is 10, to 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.3153846153846154\n"
     ]
    }
   ],
   "source": [
    "# Import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Add model step to pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', SimpleImputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('clf', RandomForestClassifier(n_estimators=15))\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At Datacamp we get, \n",
    "\n",
    "\"Accuracy on budget dataset:  .3211538461538462\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram range in scikit-learn\n",
    "Now we'll insert a CountVectorizer instance into our pipeline for the main dataset, and compute multiple n-gram features to be used in the model.\n",
    "\n",
    "In order to look for ngram relationships at multiple scales, we will use the ngram_range parameter.\n",
    "\n",
    "Special functions: we'll use a couple of new steps provided in the pipeline in this and many of the remaining exercises. Specifically, the dim_red step following the vectorizer step , and the scale step preceeding the clf (classification) step.\n",
    "\n",
    "These have been added in order to account for the fact that we're using a reduced-size sample of the full dataset. To make sure the models perform as the expert competition winner intended, we have to apply a dimensionality reduction technique, which is what the dim_red step does, and we have to scale the features to lie between -1 and 1, which is what the scale step does.\n",
    "\n",
    "The dim_red step uses a scikit-learn function called SelectKBest(), applying something called the chi-squared test to select the K \"best\" features. The scale step uses a scikit-learn function called MaxAbsScaler() in order to squash the relevant features into the interval -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.19038461538461537\n"
     ]
    }
   ],
   "source": [
    "# Import pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import other preprocessing modules\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# Select 300 best features\n",
    "chi_k = 300\n",
    "\n",
    "# Import functional utilities\n",
    "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Perform preprocessing\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', SimpleImputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                   ngram_range=(1,2))),\n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datacamp: Log loss score: 1.2681. We'll now add some additional tricks to make the pipeline even better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement interaction modeling in scikit-learn\n",
    "It's time to add interaction features to your model. The PolynomialFeatures object in scikit-learn does just that, but here we're going to use a custom interaction object, SparseInteractions. Interaction terms are a statistical tool that lets our model express what happens if two features appear together in the same row.\n",
    "\n",
    "SparseInteractions does the same thing as PolynomialFeatures, but it uses sparse matrices to do so. The code for SparseInteractions are included here from DataDriven.\n",
    "\n",
    "PolynomialFeatures and SparseInteractions both take the argument degree, which tells them what polynomial degree of interactions to compute.\n",
    "\n",
    "We're going to consider interaction terms of degree=2 in our pipeline. We will insert these steps after the preprocessing steps we've built out so far, but before the classifier steps.\n",
    "\n",
    "Pipelines with interaction terms take a while to train (since we're making n features into n-squared features!), so as long as we set it up right, we'll do the heavy lifting and tell what the score is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class SparseInteractions(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, degree=2, feature_name_separator=\"_\"):\n",
    "        self.degree = degree\n",
    "        self.feature_name_separator = feature_name_separator\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not sparse.isspmatrix_csc(X):\n",
    "            X = sparse.csc_matrix(X)\n",
    "\n",
    "        if hasattr(X, \"columns\"):\n",
    "            self.orig_col_names = X.columns\n",
    "        else:\n",
    "            self.orig_col_names = np.array([str(i) for i in range(X.shape[1])])\n",
    "\n",
    "        spi = self._create_sparse_interactions(X)\n",
    "        return spi\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "\n",
    "    def _create_sparse_interactions(self, X):\n",
    "        out_mat = []\n",
    "        self.feature_names = self.orig_col_names.tolist()\n",
    "\n",
    "        for sub_degree in range(2, self.degree + 1):\n",
    "            for col_ixs in combinations(range(X.shape[1]), sub_degree):\n",
    "                # add name for new column\n",
    "                name = self.feature_name_separator.join(self.orig_col_names[list(col_ixs)])\n",
    "                self.feature_names.append(name)\n",
    "\n",
    "                # get column multiplications value\n",
    "                out = X[:, col_ixs[0]]\n",
    "                for j in col_ixs[1:]:\n",
    "                    out = out.multiply(X[:, j])\n",
    "\n",
    "                out_mat.append(out)\n",
    "\n",
    "        return sparse.hstack([X] + out_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.31153846153846154\n"
     ]
    }
   ],
   "source": [
    "# Instantiate pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', SimpleImputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                   ngram_range=(1, 2))),  \n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('int', SparseInteractions(degree=2)),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datacamp: Log loss score: 1.2256. Nice improvement from 1.2681!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the hashing trick in scikit-learn\n",
    "Now we will check out the scikit-learn implementation of HashingVectorizer and add it to our pipeline.\n",
    "\n",
    "HashingVectorizer acts just like CountVectorizer in that it can accept token_pattern and ngram_range parameters. The important difference is that it creates hash values from the text, so that we get all the computational advantages of hashing!\n",
    "\n",
    "We've constructed a robust, powerful pipeline capable of processing training and testing data. Now that we understand the data and know all of the tools we need, we can essentially solve the whole problem in a relatively small number of lines of code. \n",
    "\n",
    "All you need to do is add the HashingVectorizer step to the pipeline to replace the CountVectorizer step.\n",
    "\n",
    "The parameters non_negative=True, norm=None, and binary=False make the HashingVectorizer perform similarly to the default settings on the CountVectorizer so we can just replace one with the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'non_negative'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-7d66460f8223>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m                                                      \u001b[0mnon_negative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                                                      \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                                                      ngram_range=(1,2))),\n\u001b[0m\u001b[0;32m     18\u001b[0m                     \u001b[1;33m(\u001b[0m\u001b[1;34m'dim_red'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchi2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchi_k\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 ]))\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'non_negative'"
     ]
    }
   ],
   "source": [
    "# Import the hashing vectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Instantiate the winning model pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', SimpleImputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC, \n",
    "                                                     non_negative=True,\n",
    "                                                     norm=None, binary=False,\n",
    "                                                     ngram_range=(1,2))),\n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('int', SparseInteractions(degree=2)),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log loss: 1.2258. Looks like the performance is about the same, but this is expected since the HashingVectorizer should work the same as the CountVectorizer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
